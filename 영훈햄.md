코드 + 이론

####### 데이터 EDA

- 모든 프로젝트의 첫 
- 분석의 방향성, 모델의 성능 향상(문제점 파악), 새로운 인사이트 발견
- 데이터 로드 -> 정제 및 전처리 -> 개별 변수 분석 -> 변수 간 관계 분석 -> 분석 결과 정리 

상관 관계
- 두 변수가 함께 변화하는 관련성 
- -1 과 1 사이의 값을 가짐
- 양의 상관관계(두 변수가 같이 증가), 음의 상관관계(두 변수가 같이 감소), 상관관계 없음(관련 X )
- 상관관계가 높다고 해서 인과관계가 있다는 것은 아님 

데이터 시각화
- Matplotlib
    - 파이썬 시각화 라이브러리의 근간
    - 세부적인 요소, 크기, 레이아웃을 설정
- Seaborn
    - 통계적으로 의미 있는 그래프를 쉽게 만들어주는 라이브러리 
    - Matplolib을 기반으로 동작 
- Matplotlib을 기반으로 제목/축/라벨등을 추가하고, Seaborn으로 데이터를 그래프로 그림

데이터 시각화 그래프 종류
- 히스토그램
    - 막대 그래프로 표현하며, 하나의 데이터 분포를 파악하기 위해 사용
- 박스 플롯
    - 사분위수를 사용하며, 이상치를 파악하기 위해 사용 
    - 사분위수
        - 25, 50, 75% 를 구하는 것
        - 이상치: 25% - 1.5*(IQR(75% - 25%))
- 산점도
    - 두 변수 사이에 점을 찍는 그래프
    - 패턴이나 이상치를 쉽게 찾을 수 있음
    - 데이터가 많으면 파악하기 어렵고, 상관관계를 보여줄 뿐, 인과관계를 증명하지 않음
- 히트맵
    - 여러 숫자형 변수들 간의 크기를 색상으로 변환해서 보여주는 것 
    - 상관관계 분석에 많이 사용 됨 
- 페어 플롯
    - 여러 변수들 간의 관계를 한 번에 보여주는 그래프 
    - 변수들 사이의 관계와 분포를 한 번에 보기 좋음 
    - 복잡해질 수 있음 

######## EDA 의 연장선

- NLP에서 중요하게 체크해야 할 EDA 항목
    1. sequence length
    2. langauge
    3. input data domain
- pad_token
    - 데이터를 효율적으로 처리하기 위해서 “가장 긴 문자의 길이에 맞춰 나머지 짧은 문장의 뒤에 채워 넣는 역할”
- eos_token
    - 문장이 어디서 끝나는지 보여주는 토큰
- 양자화(Quantization)
    - 모델의 매개변수를 더 낮은 정밀도로 표현하여 모델의 크기와 연산량을 줄이는 기술
    - 연산/메모리 부하를 줄이는 가장 직관적인 방법
- PTQ(Post-Traning Quantization)
    - 모델 학습이 완료된 후, 모델의 가중치와 화설호아 함수 값을 낮은 정밀도로 변환
    - 재학습 과정이 필요 없어서 매우 간단하지만 정확도 손실이 발생할 수 있음
- QAT(Quantization-Aware Training)
    - **모델 학습 과정**에 양자화를 추가시켜 모델이 낮은 정밀도에 적응하도록 훈련하는 방식
    - 정확도 손실이 적지만, 복잡하고 학습데이터가 필요함
- weight-only quantization vs weight-activation quantization
    - weight-only quantization(가중치 전용 양자화)
        - 가중치만 양자화하는 방식
        - 모델 크기의 대부분을 차지하는 가중치를 압축하기 때문에 공간을 크게 아낄 수 있음
        - 활성화 값은 여전히 높은 정밀로 계싼되기 때문에 추론 속도나 사용량을 개선하진 못함
    - weight-activation quantization(가중치 활성화 양자화)
        - 가중치와 활성화 값 모두를 낮은 정밀도로 양자화하는 방법
        - 모델 크기를 최대로 줄이고, 효율을 올릴 수 있음
        - 복잡하고, 모델 정확도 손실이 더 클 수 있음
- Symmetric vs Asymmetric
    - Symmetric(대칭)
        - 원점을 중심으로 양자화 범위를 설정, 최대/최소 절대값이 같도록 범위를 설정
        - 간단하지만, 데이터 분포가 비대칭적일 경우, 사용되지 않는 양자화 범위가 생겨서 정확도 손실이 생길 수 있음
    - Asymmetric(비대칭)
        - 실제 데이터의 최솟값과 최댓값에 맞춰 양자화 범위를 설정
        - 정확도 손실이 적지만, zero-point(원점)을 따로 관리해야 해서 구현이 까다로움
- Mixed-precision quantization
    - 모델의 각기 다른 부분에서 서로 다른 정밀도를 적용하는 양자화 방식
    - 효율적이고, 높은 유연성이 있음
    - 복잡하고, 하드웨어 제약이 있을 수 있음
- 양자화 - Pruning
    - 가지치기
    - 모델의 가중치 중 중요도가 낮은 연결을 제거하여 모델을 작게 만드는 기술
    - magnitude-based pruning
        - 신경망 가중치의 절대값 크기를 기준으로 중요도가 낮은 가중치를 제거
        - 가장 간단하고 일반적인 가지치기 기법
- 양자화 - Unstructured vs Structured
    - Unstructured(비정형)
        - 모델의 구조와 관계없이 개별 가중치를 제거하는 방식
        - 정확도 손실 없이 많은 가중치를 제거할 수 있음
        - 모델의 연결이 불규칙적으로 끊어지기에, 효율적인 연산이 어려움
    - Structured Pruning(정형)
        - 모델의 구조(채널, 필터, 행, 열) 단위로 가중치를 제거하는 방식
        - 효율적이지만, 모델 압축률이 낮을 수 있음
- 가지치기 인덱싱 데이터 형식
    - 불필요한 0의 가중치들을 제외하고, 유효한 값들만 저장하는 방식
    - COO(Coordinate)
        - 0이 아닌 각 원소를 (행 좌표, 열 좌표)의 형태로 저장
        - 직관적이고, 쉽게 추가/삭제가 가능
        - 인덱스 정보를 모두 저장해야 해서 메모리 사용량이 큼
    - CSR(Compressed Sparse Row) | CSC(Compressed Sparse Column)
        - 행을 기준으로 압축
        - value: 0이 아닌 값들을 순서대로 저장
        - column_indices: 각 값의 열 인덱스를 저장
        - row_pointers: 각 행의 시작 위치를 values 배열의 인덱스로 저장
        - 행 단위로 접근하기 쉽고, 벡터 곱셈 연산에 효율적
        - 특정 행의 모든 값을 읽거나 행을 삽입/삭제하는 것이 복잡함
- 지식 증류 (Knowledge Distillation)
    - **크고 똑똑한 교사 모델**의 풍부한 학습 노하우를 **작고 빠른 학생 모델**에게 전수하여, 성능은 유지하면서도 모델의 크기와 연산량을 줄이는 기술
- PEFT
    - Parameter-Efficient Fine-Tuning
    - 적은 수의 매개변수만 학습시켜 LLM을 효율적으로 파인튜닝하는 기술들의 집합
    - LoRA
- LoRA
    - Low-Rank Adaptation
    - LLM을 효율적으로 파인튜닝하는 기술
    - 적은 수의 추가적인 매개변수만 학습시켜 모델의 성능을 향상시킴
    - 기존 가중치 고정 → 새로운 작은 행렬 추가 → 미세 조정 → 추론
- LoRA - Adapter
    - 기존에 모델을 구성하고 있던 계층과는 별개의 파라미터를 사용하는 개념
    - forward 연산은 pretrained weights와 adapter를 따로 수행하고, 각 결과를 합쳐서 다음 레이어에 보냄
    - backward 연산은 pretrained weights가 필요하지 않고, adapter는 파라미터 수가 굉장히 적어서 가능
    - but 이렇게 하면, forward 추론 시 분리하기에, 학습이 끝난 adapter의 가중치를 pretrained weights에 더하면서 해결
- QLoRA
    - 경량화 + LoRA
    - 원본 모델을 강하게 양자화 → 범용 성능의 하락
    - LoRA의 작은 가중치 행렬을 학습
    - 획기적인 메모리 절감, 높은 성능, 비용 횽류성, 접근성 향상
######## 선형회귀

- 머신러닝
    - 학습 종류
        - 지도 학습
            - 정답지가 있음
            - 회귀:
                - 연속적인 숫자 값을 예측
                - 선형 회귀
            - 분류
                - 특징을 통해 분류 
                - 로지스틱 회귀 (이진 분류)
                    - 선형회귀 모델 + 시그모이드 함수(0 ~ 1) + 이진 교차 엔트로피(틀린 답에 확신하면 강한 오차)
                - 소프트맥스 회귀 ( 다중 분류)
                    - 선형회귀 + 소프트맥스(모든 클래스 확률 총합이 1) + 범주형 교차 엔트로피(정답 확률값에만 집중)

- 모델 평가
- 선형 회귀
    - MSE(평균 제곱 오차)


####### 리니어프로빙

- 학습률 스케쥴러의 종류를 지정 
                    # linear는 학습 초기에 학습률을 서서히 높였다가 점진적으로 낮추는 방식
                    lr_scheduler_type = "linear",
- 동작과정
    - 사전 학습된 모델 준비
    - 특징 추출기 고정(freeze): 모델의 몸통을 모두 학습 불가능 상태로 만듦
    - 분류기 교체: 모델의 기존 마지막 층을 제거
    - 선형 층 추가: 우리의 새로운 작업을 위한 단순한 선형 층(Linear layer)하나를 그 자리에 붙임
- probing: 탐색하다, 찔러보다
    - 사전 학습된 모델이 추출한 특징이 얼마나 좋은지, 그 위에 선형(Linear) 분류기라는 단순한 막대    기 하나만 꽂아서 찔러 보는 것
    - 만약 분류가 잘 된다면, 이 사전 학습된 모델이 특징을 잘 뽑는다고 평가할 수 있다